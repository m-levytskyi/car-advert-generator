# Architecture

This chapter describes the architecture of the system.

## Overview

[Give an overview of the architecture]

## Modules 

[Describe the modules of your system. If you added/removed/changed modules, explain why and change the chapter structure accordingly. A module description contains: Role in the overall system, input/output data, data flow, used packages, specific design decisions, ...]

### Webcam 

**Role**: Frontend interface for capturing car images  
**Data Flow**: Webcam → Image files  

**Implementation**:

- OpenCV (`cv2`) based video capture system
- Real-time preview with interactive controls
- Configurable image storage

**Design Decisions**:

- Keyboard-driven interface for simplicity:
    - Space: capture image
    - Enter: process
    - Esc: abort
- Raw image storage for preprocessing flexibility
- Configurable output paths


### Object Detector 

The object detection is divided into 2 steps:

1. Preprocessing
2. Detection on the processed image

Preprocessing means extracting only the car from the image and resizing it to 256x256 pixels. This is done by trusting YOLO11-X. \
Its' pretrained weights trained on the ImageNet dataset result in 79.5% accuracy and a mAPval50-95 on the COCO dataset of 54.7 [@yolo11_ultralytics]. \
We take all detections of class 2 - "car" and cut those out, in which the model is at least 80% confident. Then the biggest bounding box is extracted to ignore potential cars in the background. \
The extracted image now definitely contains a car and only the car and is fed into the object detector itself. \
AlexNet was trained by us to either recognize 21 popular car brands or the 5 body styles "family_car", "hatchback", "sedan", sports_car and "suv". It does so with a 86% accuracy on "brand" and 84% on "body_style", what is quite impressive. \
The preprocessing is essential at this point, as the accuracy decreases to 28% / 41% when its let out and non-extracted images are directly fed into AlexNet. The network relies heavily on the preprocessing. \

A detailed description of the training process and the object detector can be found in the [evaluation](evaluation.qmd#image-classifier).

#### AlexNet

AlexNet won the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It has started peoples interest in Deep Learning and can be seen as the beginning of the current AI boom. \
It achieves good accuracy with only 5 convolutional and 3 fully connected layers. The ReLu activation function, Dropout layers and data augmentation helped the model to perform this good. \
Still, for 2012 the network was relatively deep and could only be realized by separating the data stream over two GTX 580 GPUs with each 3GB VRAM. Being able to combine the power of 2 GPUs was also a key advancement. \
It predicts the 1000 ImageNet classes with an accuracy of 60.3%. \
[@krizhevsky2012imagenet]

#### ResNet
Residual networks make it possible to train deeper neural networks. Vanishing gradients are one of the key issues with deep neural networks. This problem could be mitigated by using skip connections in the layers enabling a better inforamtion flow through the network. This architecture was a significant improvement over previous generations. \
The original Resnet achieved an $3.57%$ error rate on ImageNet [@he2015deepresiduallearningimage]. We tried a Resnet50 v1.2 which is a modified version of the original Resnet optimized for PyTorch [@nvidia_resnet50_pytorch].

#### Visual Transformer
**Architecture**: Vision Transformer (ViT Base Patch16 224-in21k) [@dosovitskiy2020vit]

- Model: google/vit-base-patch16-224-in21k
- Patch Size: 16x16 pixels
- Resolution: 224x224
- Features:
  - Attention-based image processing
  - Pretrained transformer backbone
  - Custom classification head

**Design Decisions**:

- Configurable layer freezing for transfer learning
- Flexible classification head for different tasks
- Adam optimizer with 1e-4 learning rate
- 50 epochs training schedule


### Article Agent 

<!-- [Also describe the used tools (e.g. wikipedia) and the reason for that choice] -->
**Role**: Generate paragraphs, image titles and image descriptions from the predicted $brand$ and $bodystyle$


**Data Flow**: Brand + Bodystyle → Agent logic → JSON data with paragraphs, image titles, image descriptions

![Flowchart of the Article Agent](images/flowchart_article_agent.svg)

**LLMs**

The article agent is made up of two different LLMs, and two tools. One LLM is used for requesting information from the tools. This LLM is provided by the [API of HuggingFace](https://python.langchain.com/docs/integrations/llms/huggingface_endpoint/). The other LLM is used for the generation of text and offered by [Groq](https://python.langchain.com/docs/integrations/chat/groq/). In this project, we use the `llama3-8b-8192` model. This setup is used, because the more powerful Llama3 model by Groq used to generate the text has lower rate limits. The tool calling requires several API calls, with potentially many input tokens; this can exceed the rate limit if a number of articles are created in quick succession. Therefore, another LLM with higher rate limits is used for the tool calling, specifically the `zephyr-7b-alpha` model [by HuggingFace](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha). This model does not generate text in the quality of Llama3, but is more than capable for the tool calling. The tool calls by this model are then transferred to the text generating LLM for the final paragraphs and image titles. This way, rate limits are not an issue while not compromising on text quality.

**Tools**

The Wikipedia tool allows for general information retrieval.  For some paragraphs, the request to the LLM might look like this: "Write a paragraph for an article about a new $car_type$ offered by $brand$." Here, general information like a Wikipedia article about the brand are useful. The [Wikipedia API](https://python.langchain.com/docs/integrations/retrievers/wikipedia/) can be called with any searchstring and return a relevant article.

For more recent and specific information, the [DuckDuckGo API](https://python.langchain.com/docs/integrations/tools/ddg/) is better suited. The finished article should be tailored to a specific car, as the only information is brand and body type, DDG can help to create more variance in the article. The LLM can call this tool with a search string that is then forwarded to the DuckDuckGo API.

**Context**

If paragraphs were already created, they are included in the request. This way, if the model writes about a specific model in a paragraph, in the next paragraph no other car will be described. For each generated paragraph, the LLM then creates image descriptions and titles. Internally, the requests are refined so that images are created from the front, back and interior. 

### Article Assembler 

[Also describe potential article templates/structures you designed]

**Role**: Generate structured car articles from templates  
**Data Flow**: JSON data + Images → HTML → PDF

**Implementation**:

- Template-based article generation
- HTML/PDF conversion pipeline
- Configurable output formatting

**Template Evolution**:

1. Initial Markdown Template:
   - Simple, text-focused structure
   - Basic image placement
   - Limited styling options

2. Enhanced HTML Template:
   - Rich styling with CSS
   - Flexible layout system
   - Dark theme with accent colors
   - Responsive image positioning (left/right)
   - Custom typography and spacing

**Design Decisions**:

- HTML over Markdown for visual control
- Modular template structure
- Dark theme
- Responsive image layouts

### Diffusion Model 

[Also describe the used diffusion model]

Stable Diffusion Model is a generative model that can generate high-quality images. It is based on the diffusion process, where the model iteratively refines the image by adding noise. The model is trained on a large dataset of images and can generate realistic images of cars. The model is used to generate images of cars for the article.

First described in @rombach2021diffusion. Developed by The Machine Vision & Learning Group at LMU Munich. 

**Role**: Generate car images

**Implementation**:

- Model: CompVis/stable-diffusion-v1-4
- Hardware acceleration support

**Design Decisions**:

- Pipeline architecture for batch processing
- Automatic device selection (CUDA/CPU)
- Configurable image parameters:
  - Resolution: 512x512
  - Inference steps: 50
  - Guidance scale: 7.5
- Error handling for failed generations
- Using [compel](https://github.com/damian0815/compel/blob/main/README.md) library to handle long prompts (over 77 tokens)