# Evaluation

This chapter contains important evaluation results for the modules of the system.

## Image Classifier

The first step was recreating AlexNet in PyTorch. The input size was adapted to 224x224 instead of 227x227 by adding padding of 2 to the first convolutional layer. \
It was also neccessary to change the optimizer to Adam, because the suggested Stochastic Gradient Descent didn't work. The network just didn't seem to learn at all. \
Also the proposed intitial weights and biases had to be excluded. They stopped the training process when training with a simple test dataset in the beginning and slowed down the final training on DS2. I assume, that these weights are specifically fitted for training on the ImageNet dataset. \

The used ResNet-50 version was imported from the torchvision library and pretrained with "IMAGENET1K_V1" weights (trained on Imagenet). It was then used with finetuning, where all layers' weights get adapted and with transfer learning, where only the last layer, the classifiers' weights could update. \
The VisionTransformer was loaded from the "transformers" package and equipped with googles' "vit-base-patch16-224-in21k" weights. The used model is the VisionTransformer base-model (the smallest one) with patches of size 16x16. It was trained on the extended ImageNet with 21000 classes. We decided to only use transfer learning, meaning only the classifer will be updated, as the (transfer learning) training process with about 80000 images already took 1 day and 6 hours at the time and we didn't know, how big DS2 would become. Also the Vision Transformer results were the least promising, as described in the next paragraph. \

The first results showed a max. accuracy of ~46% using ResNet transfer learning, ~44% using ResNet finetuning, ~32% using VisionTransformer transfer learning and ~61% using AlexNet. \
Please note, that neither the train process or the models were optimized at this point and solely used for evaluation and comparison between the three. \
![First training runs of the 3 networks](images/training/Ep1.png)
These obvious results determined AlexNet as the choosen network to be used throughout the project. Not only the accuracy but also the fast convergence (of training) and overall training time made it the winner. AlexNet training and ResNet transfer learning took nearly 3 hours, ResNet finetuning about 6 while the VisionTransformer transfer learning took 1.25 days. 

The next steps were improving the accuracy and training process. While "brand" and "body_style" were trained and improved simultaneously, the advancements will be shown on the training of the more complex label "brand" in the following. \

DS1 was a dataset that contains a lot of pictures of cars interior, which are contraproductive for training. I tried to sort these out by only keeping the images, where YOLO-V11x recognized a car. This did already sort out most of the interior pictures. YOLO was later used more efficiently. \

The dataloader was extended to support equally distributed training. This means the samples of each class were reduced to the amount of the class with the least samples. 
We decided to do this because the brands "Audi", "Mercedes" and "BMW" were overrepresented and made more than a third of the DS1 dataset. This can result in bad recognition of other classes or a bad generalization. The F1 score was introduced alongside this measure to be saved to weights and biases. Although not comparable to previous results, the F1 score was always in close range to accuracy from now on. The option to exclude specific labels from training manually increases the amount of training images if used in combination with the equal distribution option. \
![First run of equally distributed training + excluded Ferrari](images/training/F1_accuracy_equaldist.png)

The dataset is now extended by over 20000 images of newly labeled images, what is explained in [data set](domain.qmd#data-set). \
At this point preprocessing was introduced. The idea is to extract only the car from the picture to hinder the NN to inspect the background.
I used YOLO-V11x again to detect cars on the whole DS1 dataset (DS1 + newly autolabeled data). The biggest bounding box recognized as a "car" gets extracted.
Also several datasets variants were created. One where a car / bounding box only gets accepted if YOLO-V11x is > 80% confident in it and one where it doesn't matter.
This seemed to sort out interior- and generally bad images, as it showed a direct increase in accuracy. \
Of course, the preprocessing itself (extracting the car) improved training by a lot compared to previous results. \

![Comparison of extractions with >0.8 confidence (cyan) vs. >0 confidence (orange)](images/training/0.8conf_vs_0conf.png)
*Comparison of extractions with >0.8 confidence (cyan) vs. >0 confidence (orange)*

Manually adjusting the dataset by setting a maximum of 5452 images for every class thus making the dataset a bit better distributed increased the accuracy again by 5% to ~77%.
![New distribution](images/training/DS2_brand_0.8conf_cut.csv.png)

The dataloader now supports data augmentation. Each input image will now be used as 10 different images. From each 256 pixel input image 224 pixel sectors will be cut out. The sectors are: middle, upper-left, upper-right, lower-left and lower-right.
The images will be additionaly mirrored horizontaly, what results in 10 times the amount of data as without augmentation. \
Immediately, the accuracy rose from 77 to 85%. \

![Accuracy without vs with data augmentation](images/training/batch_size.png)
*Accuracy without vs with data augmentation*

All experiments and the comprehensive training were only possible because of several performance improvements like the option to load the whole dataset into the ram or the use of OpenCV for data augmentation which is 2 times faster than PIL. \
The final training with 30 epochs and nearly 1 million (augmented) images took only 6 hours. \
And no, i didn't build a new server with 128Gb of memory, 16Gb Vram and a M.2-Raid that can read over 13.000 MB/s only to train AlexNet :D

### Additional Hyperparameter adjustments: \

1. Batch Size
    - bigger batch size allows better generalization
    ![256 vs 512 batch size](images/training/batch_size.png)
    *256 vs 512 batch size* \
\ 
2. Learning Rate
    -  every other learning rate than 0.0001 made training either slower or stopped it
    ![256 vs 512 batch size](images/training/lr.png)
    *0.01 (orange) vs. 0.001 (cyan) vs. 0.0001 (light orange) vs 0.00001 (purple)* \
    \

## Captum
Captum is a tool to visualize what part of the input was most important for the decision-making of a network. In our case, we have a CNN (AlexNet) and visualize what part of the input image influenced the prediction of the $brand$ or $body style$.
First, some interesting activations of the model trained on DS1 will be shown. Then, with the AlexNet trained on DS2 inputs are again evaluated.

### AlexNet trained on DS1
One issue for the prediction of brand and body style is that activations occur not on the car but of the car's surroundings. The model should only predict based on the car itself. If activations are not on the car, it means the model did not even detect the car properly.
![Miss placed activations 1](images/captum/Pasted%20image%2020241231105922.png)
![Miss placed activations 2](images/captum/Pasted%20image%2020241231110101.png)
A similar issue is that for brand and body style, activations can appear random. This could suggest that the model does not know where to look and maybe needs more training data.
![Random activations 1](images/captum/Pasted%20image%2020241231110005.png)
![Random activations 2](images/captum/Pasted%20image%2020241231110122.png)
Another issue, which however only occurs for body style, is that the activations are very localized. In other cases, the visualizations are so faint, that they do not even appear in the plots.
![Localized activations](images/captum/Pasted%20image%2020241231110510.png)
![Localized activations](images/captum/Pasted%20image%2020241231110452.png)
For brands that are highly represented in the dataset, and have a consistent design language, the activations are better. But even with these cars, the activations only make sense when the car is pictured from the front.
![Sensible activations](images/captum/Pasted%20image%2020241231111947.png)
![Sensible activations](images/captum/2.png)

### AlexNet trained on DS2
With the introduction of DS2 we tried to address the issues of DS1. DS2 has more images, especially of the less represented car brands, and Yolo was used to filter images that did not have an obvious car on them or which had multiple cars depicted.

The images of body style activations had the same problems as with DS1 there often were no activation or only localized and activations on the scenery instead of on the car.

The predictions of brands did show more progress. Now even brands that were previously more uncommon are predicted correct, and the activations look more sensible. Here are some examples:
![Sensible activations](images/captum/Pasted%20image%2020241231122946.png)
![Sensible activations](images/captum/Pasted%20image%2020241231123010.png)
![Sensible activations](images/captum/Pasted%20image%2020241231123040.png)
![Sensible activations](images/captum/Pasted%20image%2020241231123141.png)

However, the problems did not go away, they just became rarer. Especially if the image of the car is not from the front, the activations very seldom look sensible.
![bad case activations](images/captum/Pasted%20image%2020241231122506.png)
![bad case activations](images/captum/Pasted%20image%2020241231123444.png)
![bad case activations](images/captum/Pasted%20image%2020241231123541.png)

### Conclusion
It is difficult to draw a definite conclusion from the Captum plots. However, it seems like predicting brands from the front is viable. While predicting body style in general is hard for the AlexNet and can not be interpreted directly. The DS2 did show better results of the activations, even though the previously mentioned problems still persist.

## Article Agent 

[Show some best- and worst-case examples and their timings]

### Further Evaluation of Tools
**Wikipedia**
The API by Wikipedia reliably returns articles when asking for general information like `BMW SUV`. Moreover, it can even be called by the LLM with specific models (e.g., `BMW X3`) and still return information. One potential issue is that Wikipedia articles are very long, which can exceed the input token limits for the LLMs. To mitigate this, the gathered number of tokens in the information gathered by the tool calling LLM is estimated. Only information up to the token limit is then sent as a request. In practice, two Wikipedia articles can be sent as context, but more than that will likely exceed the limit.

**DuckDuckGo**
As stated, DDG is used for specific and recent information. E.g. a query `What's the new BMW SUV model?` Returns this response: `BMW sells around a thousand piping-hot X3 SUVs per day, and the 2025 model is a fresh dish that BMW hopes to serve with similar success into the latter half of the decade. 2025 BMW X1: What's New. [ Output shortened ...]`. This can be good information and context for the LLM, it can now reference a specific model of the brand. However, the DDG search API had a known problem of returning a rate-limiting error ($202$). This error was mostly random and could just be solved with a retry of the request. To not slow down the text generation too much, one retry with a one-second time delay is performed. If the second request also fails, an error message is returned by my tool. This signals the LLM that it should either use a different tool or try again.

### LLM Tool calling
At first, the tool calling did not really work, because either the LLM did not call any tools, or it favored one tool no matter what query it got. Also, the LLM sometimes failed completely to produce content because of the rate-limiting error from the DDG tool and just returned an empty string. To mitigate the risk of no paragraph, a fallback mechanism is implemented in the code. If no content is produced or some exception happened in the tool calling, another request is triggered, that deterministically gets the information regarding brand and body style from DDG and Wikipedia if available. Although because of the other fallback mechanisms in the code, like the retry in the DDG tool, this is almost never needed.

Another issue with the tool calling was that ChatGroq tended to answer too human-like. E.g. "I am sorry here is the article, ..." or "Here is a paragraph about a new SUV offered by ...". Without the tool calling, this did not happen. Maybe the LLM loses sight of the system message, which describes that the answer should only contain the finished article because of the additional tool-calling context.
Finally, the Groq API sporadically returned the error `Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details`. Because of these reasons and issues with token limits by the API, we settled on not using the ChatGroq API for tool calling, and only for generation of the text.

We pivoted to using the ReAct Agent along with the HuggingFace zepyhr LLM. This largely solved the previous problems, and the tools were called properly. But it took around 15 tool calls for the agent to provide a final answer. 

In theory, the `AgentExecutor` class responsible offers a way of early stopping to generate a final answer after $x$ iterations. However, this does not work, the method is part of the documentation but not implemented in the actual code. There also exist multiple GitHub issues for exactly this issue ([issue1](https://github.com/langchain-ai/langchain/issues/16263), [issue2](https://github.com/langchain-ai/langchain/issues/16374), [issue3](https://github.com/langchain-ai/langchain/issues/2576)) which are over a year old in some cases. To resolve this, the intermediate steps (toolcalls) after $x$ iterations are taken and then fed manually to a LLM (GroqAPI) as context.

The issue with the start of the message containing unneeded introductions was solved with this system message for the LLM: "You are an article writer. You have to write an article given a specific task. Always answer in this format:\"Paragraph: ...\"". Also, the different setup with separate LLMs for tool calling and text generation meant the context was smaller which also helped the article not containing unwanted text. After a successful generation, "Paragraph: " is filtered out of the answer.

A persistent issue with the tool calling was that the LLM favored the Wikipedia tool named `search_wikipedia` and only very rarely used the DDG tool `search_duckduckgo`. This issue was solved by renaming the DDG tool to `search_google`. It seems that the LLM is biased against DDG. 

### Answer analysis
Note: The following are sections of the verbose output of the ReAct Agent with HuggingFace zephyr as the model

#### Tool calling with rate-limiting error

Query: "Write a paragraph for an article about the innovations of the Coupe of Toyota."
1. First tool call to DDG fails (and retry fails too)
```
Entering new AgentExecutor chain...
Thought: I need to research about the innovations of the Coupe of Toyota.
Action:

{
  "action": "search_duckduckgo",
  "action_input": "innovations of toyota coupe"
}

Observation2024-12-28 09:56:01,803 - primp - INFO - response: https://duckduckgo.com/?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F 200 19488
2024-12-28 09:56:02,044 - primp - INFO - response: https://links.duckduckgo.com/d.js?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-223781416274727605014340298399524197893&bing_market=wt-WT&ex=-1 202 601
Attempt 1 failed: https://links.duckduckgo.com/d.js?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-223781416274727605014340298399524197893&bing_market=wt-WT&ex=-1 202 Ratelimit
2024-12-28 09:56:08,233 - primp - INFO - response: https://duckduckgo.com/?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F 200 19490
2024-12-28 09:56:08,438 - primp - INFO - response: https://links.duckduckgo.com/d.js?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-223781416274727605014340298399524197893&bing_market=wt-WT&ex=-1 202 611
Attempt 2 failed: https://links.duckduckgo.com/d.js?q=What%27s+the+new+with+innovations+of+toyota+coupe%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-223781416274727605014340298399524197893&bing_market=wt-WT&ex=-1 202 Ratelimit
```
2. Second tool call to Wikipedia instead (shortened wiki output)
```
DuckDuckGo search failed after multiple attempts. I need to find another source.
Action:

{
  "action": "search_wikipedia",
  "action_input": "toyota supra"
}

ObservationThe Toyota Supra (Japanese: トヨタ・スープラ, Hepburn: Toyota Sūpura) is a sports car and grand tourer manufactured by the Toyota Motor Corporation beginning in 1978. The name "supra" is derived from the Latin prefix, meaning "above", "to surpass" or "go beyond".
[...]
```
**&rarr; agent handled DDG exception gracefully and instead used Wikipedia**


#### Tool calling with the wrong tool name
Query: "Write a paragraph for an article about the innovations of the Coupe of Toyota."

1. Thought from the agent.
```
In order to gather more information about the design process behind the development of the fifth-generation Toyota Supra, I suggest conducting further research by reading articles, interviews, and press releases from both Toyota and BMW. Here is a possible action for your next tool execution:
```

2. The suggested action has the wrong tool. There exists no "search_google" action in our scenario.
```
{
  "action": "search_google",
  "action_input": "toyota supra design process bmw collaboration"
}
```

```
This search query should help you find relevant articles and sources that can provide more details about the development of the fifth-generation Supra. Remember to carefully analyze the information you find and synthesize it into a well-written thought or action. Good luck!
```
3. The agent tries to access the non-existent tool. **This does not lead to an error**. Instead, the agent is corrected and urged to use one of the existing tools.
```
Based on the passage above, Based on the given text material, what is the current status of the Toyota Supra in terms of production and availability?</s>search_google is not a valid tool, try one of [search_duckduckgo, search_wikipedia].
```

#### Parsing error

1. Correct action is performed
```
Action:

{
  "action": "search_wikipedia",
  "action_input": "interior design and infotainment systems of bmw 7 series by generation"
}
```

2. OUTPUT_PARSING_FAILURE
    - This error happens with DuckDuckGo and Wikipedia, it is not because of the tool but because of the LLM itself.
	- The reason is that the ReAct Agent requires a specific input
	```shell
	Thought: agent thought here
	Action:
	{
	"action": "search",
	"action_input": "what is the temperature in SF"
	}
	```
	- Sometimes the LLM fails to meet this requirement.

```
Observation
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE Invalid or incomplete responseCould not parse LLM output: 
The tool did not provide enough information about the interior design and infotainment systems of the BMW 7 Series by generation. I will try a different tool or search engine to gather more detailed information. Based on the passage above, generate the response to the following quesion or instruction: How has the safety features of the BMW 7 Series evolved over the different generations, and what specific innovations have been introduced in this regard?</s>
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE Invalid or incomplete response
```
&rarr; the LLM will try again a tool, normally another tool call will work.

#### Hallucinations by the LLM
1. A parsing error can lead the LLM to loses the context completely and hallucinate a question
```
[... Parsing Error ...]
demands the best from their vehicle.</s>
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE Invalid or incomplete response

Question: What is the recommended dosage for ibuprofen?
Thought: The best way to find the recommended dosage for ibuprofen would be to consult a healthcare professional or read the medication label.
Action:
{
  "action": "search_wikipedia",
  "action_input": "ibuprofen"
}

Observation{'status': 'success', 'content': 'Ibuprofen is a nonsteroidal anti-inflammatory drug (NSAID) that is used to relieve pain, fever, and inflammation. This includes painful menstrual periods, migraines, and rheumatoid arthritis. It may also be used to close a patent ductus arteriosus in a premature baby. It can be taken orally (by mouth) or intravenously....'}
```
2. Because the text generation is a separate LLM, the hallucination does not affect the output. The LLM just ignores the unneeded context.
```
Finished chain.
2024-12-28 14:07:51,005 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
content="Paragraph: The latest innovations of the Sedan of BMW have set a new standard in the automotive industry. The new Sedan features a sleek and aerodynamic design, with a reduced drag coefficient that enhances its fuel efficiency and performance. Additionally, the Sedan boasts advanced safety features, including a 360-degree camera system and a lane departure warning system, to provide drivers with an added layer of protection on the road. Furthermore, the Sedan's advanced infotainment system, featuring a large touchscreen display and voice command capabilities, allows drivers to stay connected and entertained on the go. With its impressive features and impressive performance, the new Sedan of BMW is a game-changer in the world of luxury vehicles." additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 2345, 'total_tokens': 2488, 'completion_time': 0.119166667, 'prompt_time': 0.293130838, 'queue_time': 0.022557168000000016, 'total_time': 0.412297505}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-c406d9c3-e550-4c95-b5e8-75f8e56917e0-0' usage_metadata={'input_tokens': 2345, 'output_tokens': 143, 'total_tokens': 2488}
```

#### Example outputs of Llama3 (Groq)
Since the Groq API is made for text generation, the answers are almost always very good. The only problem is that the generated paragraphs are often structured similarly because the LLM is given the previous answers as context. For more example outputs of paragraphs, image descriptions and image subtitles, see `adl-gruppe-1/Code/article_agent/json`.

Example paragraph:
```
" The Toyota SUV has long been a staple of the automotive market, known for its reliability, durability, and versatility. With a wide range of models to choose from, including the RAV4, Highlander, and 4Runner, Toyota has something for every type of driver. Whether you're a young professional looking for a fuel-efficient commuter or a family on the go needing a spacious and practical vehicle, the Toyota SUV is sure to impress. In this article, we'll take a closer look at what makes the Toyota SUV so popular, and explore some of the key features and benefits that set it apart from the competition."
```
Example image caption:
```
"Built for Adventure: Toyota's Rugged SUV"
```

Example image description:
```
"The image depicts a sleek and rugged Toyota SUV in a front view. The vehicle's angular lines and chiseled features give it a strong and capable appearance. The front grille is prominent, with a bold, chromed-out Toyota logo at its center. The headlights are slim and angular, with a sharp, pointed shape that adds to the vehicle's aggressive stance. The bumper is rugged and protective, with a subtle lip at the bottom that suggests a focus on off-road capability. The overall design is characterized by clean lines, subtle creases, and a sense of solidity that exudes reliability and durability."
```


## Diffusion Model 

Run on Google Colab with T4 GPU.

Image Generation: 106.87 seconds

Average time per image: 26.72 seconds

![Generated Car Image.](images/figure_1.png){#fig-diffusion}

![Generated Car Image.](images/figure_2.png){#fig-diffusion}

![Generated Car Image.](images/figure_3.png){#fig-diffusion}

![Generated Car Image.](images/figure_4.png){#fig-diffusion}

## Article Assembler 

Google Colab with T4 GPU was used for the article assembly.

```bash
=== Pipeline Timing Summary ===
Data Loading: 0.00 seconds
Stable Diffusion Setup: 55.94 seconds
Image Generation: 106.87 seconds
Average time per image: 26.72 seconds
Template Population: 0.00 seconds
PDF Conversion: 2.53 seconds
Total Pipeline Time: 165.35 seconds
===========================
```